{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hy822\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import gensim\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    token_list=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 :\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            token_list.append(token)\n",
    "    return result,dict(zip(result,token_list))\n",
    "def produce_mapping(mapping_list):\n",
    "    #processed_ser= corpus.fillna(\"\").apply(lambda x: re.sub(r\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\",\" \",x)).map(preprocess)\n",
    "    #processed_docs=[item[0] for item in processed_ser]\n",
    "    #mapping_list=[item[1] for item in processed_ser]\n",
    "    mapping_pairs=pd.concat([pd.DataFrame([(k,v) for k,v in d.items()]) for d in mapping_list])\n",
    "    mapping_pairs['count']=1\n",
    "    mapping121=mapping_pairs.groupby(by=[0,1]).count().reset_index().sort_values(by=[0,'count'],ascending=False).groupby(by=0).head(1)\n",
    "    mapping12many=mapping_pairs.groupby(by=[0,1]).count().reset_index().sort_values(by=[0,'count'],ascending=False)\n",
    "    return mapping121,mapping12many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multinomial\n",
    "from numpy import log, exp\n",
    "from numpy import argmax\n",
    "import json\n",
    "\n",
    "class MovieGroupProcess:\n",
    "    def __init__(self, K=8, alpha=0.1, beta=0.1, n_iters=30):\n",
    "        '''\n",
    "        A MovieGroupProcess is a conceptual model introduced by Yin and Wang 2014 to\n",
    "        describe their Gibbs sampling algorithm for a Dirichlet Mixture Model for the\n",
    "        clustering short text documents.\n",
    "        Reference: http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
    "        Imagine a professor is leading a film class. At the start of the class, the students\n",
    "        are randomly assigned to K tables. Before class begins, the students make lists of\n",
    "        their favorite films. The teacher reads the role n_iters times. When\n",
    "        a student is called, the student must select a new table satisfying either:\n",
    "            1) The new table has more students than the current table.\n",
    "        OR\n",
    "            2) The new table has students with similar lists of favorite movies.\n",
    "        :param K: int\n",
    "            Upper bound on the number of possible clusters. Typically many fewer\n",
    "        :param alpha: float between 0 and 1\n",
    "            Alpha controls the probability that a student will join a table that is currently empty\n",
    "            When alpha is 0, no one will join an empty table.\n",
    "        :param beta: float between 0 and 1\n",
    "            Beta controls the student's affinity for other students with similar interests. A low beta means\n",
    "            that students desire to sit with students of similar interests. A high beta means they are less\n",
    "            concerned with affinity and are more influenced by the popularity of a table\n",
    "        :param n_iters:\n",
    "        '''\n",
    "        self.K = K\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.n_iters = n_iters\n",
    "\n",
    "        # slots for computed variables\n",
    "        self.number_docs = None\n",
    "        self.vocab_size = None\n",
    "        self.cluster_doc_count = [0 for _ in range(K)]\n",
    "        self.cluster_word_count = [0 for _ in range(K)]\n",
    "        self.cluster_word_distribution = [{} for i in range(K)]\n",
    "\n",
    "    @staticmethod\n",
    "    def from_data(K, alpha, beta, D, vocab_size, cluster_doc_count, cluster_word_count, cluster_word_distribution):\n",
    "        '''\n",
    "        Reconstitute a MovieGroupProcess from previously fit data\n",
    "        :param K:\n",
    "        :param alpha:\n",
    "        :param beta:\n",
    "        :param D:\n",
    "        :param vocab_size:\n",
    "        :param cluster_doc_count:\n",
    "        :param cluster_word_count:\n",
    "        :param cluster_word_distribution:\n",
    "        :return:\n",
    "        '''\n",
    "        mgp = MovieGroupProcess(K, alpha, beta, n_iters=30)\n",
    "        mgp.number_docs = D\n",
    "        mgp.vocab_size = vocab_size\n",
    "        mgp.cluster_doc_count = cluster_doc_count\n",
    "        mgp.cluster_word_count = cluster_word_count\n",
    "        mgp.cluster_word_distribution = cluster_word_distribution\n",
    "        return mgp\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample(p):\n",
    "        '''\n",
    "        Sample with probability vector p from a multinomial distribution\n",
    "        :param p: list\n",
    "            List of probabilities representing probability vector for the multinomial distribution\n",
    "        :return: int\n",
    "            index of randomly selected output\n",
    "        '''\n",
    "        return [i for i, entry in enumerate(multinomial(1, p)) if entry != 0][0]\n",
    "\n",
    "    def fit(self, docs, vocab_size):\n",
    "        '''\n",
    "        Cluster the input documents\n",
    "        :param docs: list of list\n",
    "            list of lists containing the unique token set of each document\n",
    "        :param V: total vocabulary size for each document\n",
    "        :return: list of length len(doc)\n",
    "            cluster label for each document\n",
    "        '''\n",
    "        alpha, beta, K, n_iters, V = self.alpha, self.beta, self.K, self.n_iters, vocab_size\n",
    "\n",
    "        D = len(docs)\n",
    "        self.number_docs = D\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # unpack to easy var names\n",
    "        m_z, n_z, n_z_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution\n",
    "        cluster_count = K\n",
    "        d_z = [None for i in range(len(docs))]\n",
    "\n",
    "        # initialize the clusters\n",
    "        for i, doc in enumerate(docs):\n",
    "\n",
    "            # choose a random  initial cluster for the doc\n",
    "            z = self._sample([1.0 / K for _ in range(K)])\n",
    "            d_z[i] = z\n",
    "            m_z[z] += 1\n",
    "            n_z[z] += len(doc)\n",
    "\n",
    "            for word in doc:\n",
    "                #print(word)\n",
    "                if word not in n_z_w[z]:\n",
    "                    n_z_w[z][word] = 0\n",
    "                n_z_w[z][word] += 1\n",
    "        #print(n_z_w)\n",
    "        for _iter in range(n_iters):\n",
    "            total_transfers = 0\n",
    "\n",
    "            for i, doc in enumerate(docs):\n",
    "\n",
    "                # remove the doc from it's current cluster\n",
    "                z_old = d_z[i]\n",
    "\n",
    "                m_z[z_old] -= 1\n",
    "                n_z[z_old] -= len(doc)\n",
    "\n",
    "                for word in doc:\n",
    "                    n_z_w[z_old][word] -= 1\n",
    "\n",
    "                    # compact dictionary to save space\n",
    "                    if n_z_w[z_old][word] == 0:\n",
    "                        del n_z_w[z_old][word]\n",
    "\n",
    "                # draw sample from distribution to find new cluster\n",
    "                p = self.score(doc)\n",
    "                z_new = self._sample(p)\n",
    "\n",
    "                # transfer doc to the new cluster\n",
    "                if z_new != z_old:\n",
    "                    total_transfers += 1\n",
    "\n",
    "                d_z[i] = z_new\n",
    "                m_z[z_new] += 1\n",
    "                n_z[z_new] += len(doc)\n",
    "\n",
    "                for word in doc:\n",
    "                    if word not in n_z_w[z_new]:\n",
    "                        n_z_w[z_new][word] = 0\n",
    "                    n_z_w[z_new][word] += 1\n",
    "\n",
    "            cluster_count_new = sum([1 for v in m_z if v > 0])\n",
    "            print(\"In stage %d: transferred %d clusters with %d clusters populated\" % (\n",
    "            _iter, total_transfers, cluster_count_new))\n",
    "            if total_transfers == 0 and cluster_count_new == cluster_count and _iter>25:\n",
    "                print(\"Converged.  Breaking out.\")\n",
    "                break\n",
    "            self.cluster_count = cluster_count_new\n",
    "        self.cluster_word_distribution = n_z_w\n",
    "        return d_z\n",
    "\n",
    "    def score(self, doc):\n",
    "        '''\n",
    "        Score a document\n",
    "        Implements formula (3) of Yin and Wang 2014.\n",
    "        http://dbgroup.cs.tsinghua.edu.cn/wangjy/papers/KDD14-GSDMM.pdf\n",
    "        :param doc: list[str]: The doc token stream\n",
    "        :return: list[float]: A length K probability vector where each component represents\n",
    "                              the probability of the document appearing in a particular cluster\n",
    "        '''\n",
    "        alpha, beta, K, V, D = self.alpha, self.beta, self.K, self.vocab_size, self.number_docs\n",
    "        m_z, n_z, n_z_w = self.cluster_doc_count, self.cluster_word_count, self.cluster_word_distribution\n",
    "\n",
    "        p = [0 for _ in range(K)]\n",
    "\n",
    "        #  We break the formula into the following pieces\n",
    "        #  p = N1*N2/(D1*D2) = exp(lN1 - lD1 + lN2 - lD2)\n",
    "        #  lN1 = log(m_z[z] + alpha)\n",
    "        #  lN2 = log(D - 1 + K*alpha)\n",
    "        #  lN2 = log(product(n_z_w[w] + beta)) = sum(log(n_z_w[w] + beta))\n",
    "        #  lD2 = log(product(n_z[d] + V*beta + i -1)) = sum(log(n_z[d] + V*beta + i -1))\n",
    "\n",
    "        lD1 = log(D - 1 + K * alpha)\n",
    "        doc_size = len(doc)\n",
    "        for label in range(K):\n",
    "            lN1 = log(m_z[label] + alpha)\n",
    "            lN2 = 0\n",
    "            lD2 = 0\n",
    "            for word in doc:\n",
    "                lN2 += log(n_z_w[label].get(word, 0) + beta)\n",
    "            for j in range(1, doc_size +1):\n",
    "                lD2 += log(n_z[label] + V * beta + j - 1)\n",
    "            p[label] = exp(lN1 - lD1 + lN2 - lD2)\n",
    "\n",
    "        # normalize the probability vector\n",
    "        pnorm = sum(p)\n",
    "        pnorm = pnorm if pnorm>0 else 1\n",
    "        return [pp/pnorm for pp in p]\n",
    "\n",
    "    def choose_best_label(self, doc):\n",
    "        '''\n",
    "        Choose the highest probability label for the input document\n",
    "        :param doc: list[str]: The doc token stream\n",
    "        :return:\n",
    "        '''\n",
    "        p = self.score(doc)\n",
    "        return argmax(p),max(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extrame(topic_list):\n",
    "    occurence_list=[i[1] for j in topic_list for i in j]\n",
    "    lambda_=np.mean(occurence_list)\n",
    "    std=np.std(occurence_list)\n",
    "    print(lambda_,std)\n",
    "    return [i[0] for j in topic_list for i in j if i[1]>lambda_+3*std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_topic_df=pd.read_csv(\"../data/2030_goals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155,)\n",
      "[[0.17732848581427751, 'impact', 'climate change'], [0.19626488261794026, 'level', 'climate change'], [0.20638025823456402, 'need', 'climate change'], [0.29439732392691037, 'vulner', 'climate change'], [0.41276051646912804, 'climat', 'climate change']]\n",
      "(155,)\n",
      "[[0.23792061428269928, 'digit', 'crisis and disasters'], [0.263327469470912, 'conflict', 'crisis and disasters'], [0.3172274857102657, 'crisi', 'crisis and disasters'], [0.32915933683864, 'vulner', 'crisis and disasters'], [0.460823071574096, 'disast', 'crisis and disasters']]\n",
      "(155,)\n",
      "[[0.16263104405109777, 'access', 'health'], [0.1959196938565656, 'threat', 'health'], [0.244899617320707, 'care', 'health'], [0.34285946424898983, 'diseas', 'health'], [0.6911819372171655, 'health', 'health']]\n",
      "(155,)\n",
      "[[0.21424347417799094, 'ident', 'migration and identity'], [0.21424347417799094, 'servic', 'migration and identity'], [0.23712191596358623, 'societi', 'migration and identity'], [0.35707245696331824, 'inclus', 'migration and identity'], [0.49990143974864554, 'migrat', 'migration and identity']]\n",
      "(155,)\n",
      "[[0.17516915560584528, 'time', 'values, power and inclusion'], [0.1834804479516891, 'countri', 'values, power and inclusion'], [0.1834804479516891, 'human', 'values, power and inclusion'], [0.26275373340876795, 'demand', 'values, power and inclusion'], [0.43792288901461324, 'humanitarian', 'values, power and inclusion']]\n"
     ]
    }
   ],
   "source": [
    "processed_ser=given_topic_df['description'].map(preprocess)\n",
    "processed_docs=[item[0] for item in processed_ser]\n",
    "mapping_list=[item[1] for item in processed_ser]\n",
    "\n",
    "mapping121_description,mapping12many_description=produce_mapping(mapping_list)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_transform=TfidfVectorizer(max_df=0.95, min_df=2,ngram_range=(1,1))\n",
    "\n",
    "tf = tf_transform.fit_transform([\" \".join(i) for i in processed_docs])\n",
    "\n",
    "tf_transform.get_feature_names()\n",
    "\n",
    "from numpy import argmax\n",
    "description_list=[]\n",
    "index=0\n",
    "for i in np.array(tf.todense()):\n",
    "    print(i.shape)\n",
    "    print([list(j)+[given_topic_df['topic'][index]] for j in sorted(list(zip(i,tf_transform.get_feature_names())))[-5:]])\n",
    "    description_list+=[list(j)+[given_topic_df['topic'][index]] for j in sorted(list(zip(i,tf_transform.get_feature_names())))[-5:]]\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_df=pd.DataFrame(description_list,columns=['score',\"stem\",'topic'])\n",
    "description_df['type']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141,)\n",
      "[[0.15151028541006095, 'need', 'climate change'], [0.2020137138800813, 'vulner', 'climate change'], [0.21697076492751724, 'adapt', 'climate change'], [0.420245780875786, 'environment', 'climate change'], [0.4545308562301829, 'climat', 'climate change']]\n",
      "(141,)\n",
      "[[0.24410330876667222, 'event', 'crisis and disasters'], [0.24410330876667222, 'technolog', 'crisis and disasters'], [0.2532847462881602, 'right', 'crisis and disasters'], [0.29829955887367765, 'increas', 'crisis and disasters'], [0.383528004266157, 'disast', 'crisis and disasters']]\n",
      "(141,)\n",
      "[[0.1552371882394348, 'servic', 'health'], [0.1552371882394348, 'social', 'health'], [0.17181451918368937, 'access', 'health'], [0.20698291765257976, 'care', 'health'], [0.776185941197174, 'health', 'health']]\n",
      "(141,)\n",
      "[[0.22042686685007082, 'help', 'migration and identity'], [0.22042686685007082, 'ident', 'migration and identity'], [0.22042686685007082, 'inclus', 'migration and identity'], [0.22042686685007082, 'safe', 'migration and identity'], [0.3306403002751062, 'societi', 'migration and identity']]\n",
      "(141,)\n",
      "[[0.23881716844773226, 'decis', 'values, power and inclusion'], [0.23881716844773226, 'divers', 'values, power and inclusion'], [0.23881716844773226, 'women', 'values, power and inclusion'], [0.2779426478987169, 'chang', 'values, power and inclusion'], [0.39802861407955376, 'inclus', 'values, power and inclusion']]\n"
     ]
    }
   ],
   "source": [
    "processed_ser=given_topic_df['strategy'].map(preprocess)\n",
    "processed_docs=[item[0] for item in processed_ser]\n",
    "mapping_list=[item[1] for item in processed_ser]\n",
    "\n",
    "mapping121_strategy,mapping12many_strategy=produce_mapping(mapping_list)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_transform=TfidfVectorizer(max_df=0.95, min_df=2,ngram_range=(1,1))\n",
    "\n",
    "tf = tf_transform.fit_transform([\" \".join(i) for i in processed_docs])\n",
    "\n",
    "tf_transform.get_feature_names()\n",
    "\n",
    "from numpy import argmax\n",
    "strategy_list=[]\n",
    "index=0\n",
    "for i in np.array(tf.todense()):\n",
    "    print(i.shape)\n",
    "    print([list(j)+[given_topic_df['topic'][index]] for j in sorted(list(zip(i,tf_transform.get_feature_names())))[-5:]])\n",
    "    strategy_list+=[list(j)+[given_topic_df['topic'][index]] for j in sorted(list(zip(i,tf_transform.get_feature_names())))[-5:]]\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crisis and disasters'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_topic_df['topic'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_df=pd.DataFrame(strategy_list,columns=['score',\"stem\",'topic'])\n",
    "strategy_df['type']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247,)\n",
      "[[0.23515185385375167, 'human', 'climate change'], [0.31353580513833557, 'action', 'climate change'], [0.3253288013913224, 'climat', 'climate change'], [0.3284122126221841, 'rise', 'climate change'], [0.47030370770750335, 'risk', 'climate change']]\n",
      "(247,)\n",
      "[[0.19676326803836552, 'ifrc', 'crisis and disasters'], [0.2099973687752131, 'peopl', 'crisis and disasters'], [0.25298134462075567, 'digit', 'crisis and disasters'], [0.25298134462075567, 'work', 'crisis and disasters'], [0.5059626892415113, 'conflict', 'crisis and disasters']]\n",
      "(247,)\n",
      "[[0.15732994173818404, 'potenti', 'health'], [0.18310534742828694, 'emerg', 'health'], [0.21766343972981356, 'rapid', 'health'], [0.47198982521455213, 'diseas', 'health'], [0.5768764530400081, 'health', 'health']]\n",
      "(247,)\n",
      "[[0.1732019794332808, 'technolog', 'migration and identity'], [0.20865437459319133, 'middl', 'migration and identity'], [0.2549801893824636, 'societi', 'migration and identity'], [0.30310346400824145, 'engag', 'migration and identity'], [0.43710889608422326, 'communiti', 'migration and identity']]\n",
      "(247,)\n",
      "[[0.21144212048633573, 'network', 'values, power and inclusion'], [0.23402141380264252, 'organ', 'values, power and inclusion'], [0.23402141380264252, 'trust', 'values, power and inclusion'], [0.2819228273151143, 'influenc', 'values, power and inclusion'], [0.3691241079563793, 'govern', 'values, power and inclusion']]\n"
     ]
    }
   ],
   "source": [
    "processed_ser=given_topic_df['extra'].map(preprocess)\n",
    "processed_docs=[item[0] for item in processed_ser]\n",
    "mapping_list=[item[1] for item in processed_ser]\n",
    "\n",
    "mapping121_extra,mapping12many_extra=produce_mapping(mapping_list)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_transform=TfidfVectorizer(max_df=0.95, min_df=2,ngram_range=(1,1))\n",
    "\n",
    "tf = tf_transform.fit_transform([\" \".join(i) for i in processed_docs])\n",
    "\n",
    "tf_transform.get_feature_names()\n",
    "\n",
    "from numpy import argmax\n",
    "extra_list=[]\n",
    "index=0\n",
    "for i in np.array(tf.todense()):\n",
    "    print(i.shape)\n",
    "    print([list(j)+[given_topic_df['topic'][index]] for j in sorted(list(zip(i,tf_transform.get_feature_names())))[-5:]])\n",
    "    extra_list+=[list(j)+[given_topic_df['topic'][index]] for j in sorted(list(zip(i,tf_transform.get_feature_names())))[-5:]]\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df=pd.DataFrame(extra_list,columns=['score',\"stem\",'topic'])\n",
    "extra_df['type']=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_topic=pd.concat([description_df,strategy_df,extra_df],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>stem</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.177328</td>\n",
       "      <td>impact</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.196265</td>\n",
       "      <td>level</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.206380</td>\n",
       "      <td>need</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.294397</td>\n",
       "      <td>vulner</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.412761</td>\n",
       "      <td>climat</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.211442</td>\n",
       "      <td>network</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>organ</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>trust</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.281923</td>\n",
       "      <td>influenc</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.369124</td>\n",
       "      <td>govern</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score      stem                        topic  type\n",
       "0   0.177328    impact               climate change     1\n",
       "1   0.196265     level               climate change     1\n",
       "2   0.206380      need               climate change     1\n",
       "3   0.294397    vulner               climate change     1\n",
       "4   0.412761    climat               climate change     1\n",
       "..       ...       ...                          ...   ...\n",
       "70  0.211442   network  values, power and inclusion     3\n",
       "71  0.234021     organ  values, power and inclusion     3\n",
       "72  0.234021     trust  values, power and inclusion     3\n",
       "73  0.281923  influenc  values, power and inclusion     3\n",
       "74  0.369124    govern  values, power and inclusion     3\n",
       "\n",
       "[75 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_fb_df=pd.read_csv(\"../data/facebook_Malawi.csv\",delimiter=\"|\",index_col=0)\n",
    "MI_tw_df=pd.read_csv(\"../data/twitter_Malawi.csv\",delimiter=\"|\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ser = MI_fb_df['message'].fillna(\"\").apply(lambda x: re.sub(r\"http[s]?\\:\\/\\/.[a-zA-Z0-9\\.\\/\\_?=%&#\\-\\+!]+\",\" \",x)).map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs=[item[0] for item in processed_ser]\n",
    "mapping_list=[item[1] for item in processed_ser]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping121_MI,mapping12many_MI=produce_mapping(mapping_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 247 clusters with 25 clusters populated\n",
      "In stage 1: transferred 84 clusters with 19 clusters populated\n",
      "In stage 2: transferred 30 clusters with 17 clusters populated\n",
      "In stage 3: transferred 25 clusters with 16 clusters populated\n",
      "In stage 4: transferred 27 clusters with 17 clusters populated\n",
      "In stage 5: transferred 26 clusters with 16 clusters populated\n",
      "In stage 6: transferred 26 clusters with 14 clusters populated\n",
      "In stage 7: transferred 21 clusters with 15 clusters populated\n",
      "In stage 8: transferred 19 clusters with 15 clusters populated\n",
      "In stage 9: transferred 16 clusters with 14 clusters populated\n",
      "In stage 10: transferred 21 clusters with 15 clusters populated\n",
      "In stage 11: transferred 25 clusters with 16 clusters populated\n",
      "In stage 12: transferred 26 clusters with 15 clusters populated\n",
      "In stage 13: transferred 26 clusters with 14 clusters populated\n",
      "In stage 14: transferred 26 clusters with 15 clusters populated\n",
      "In stage 15: transferred 29 clusters with 15 clusters populated\n",
      "In stage 16: transferred 25 clusters with 17 clusters populated\n",
      "In stage 17: transferred 24 clusters with 17 clusters populated\n",
      "In stage 18: transferred 24 clusters with 15 clusters populated\n",
      "In stage 19: transferred 20 clusters with 16 clusters populated\n",
      "In stage 20: transferred 14 clusters with 13 clusters populated\n",
      "In stage 21: transferred 15 clusters with 14 clusters populated\n",
      "In stage 22: transferred 18 clusters with 15 clusters populated\n",
      "In stage 23: transferred 15 clusters with 14 clusters populated\n",
      "In stage 24: transferred 13 clusters with 14 clusters populated\n",
      "In stage 25: transferred 18 clusters with 17 clusters populated\n",
      "In stage 26: transferred 19 clusters with 13 clusters populated\n",
      "In stage 27: transferred 20 clusters with 14 clusters populated\n",
      "In stage 28: transferred 26 clusters with 15 clusters populated\n",
      "In stage 29: transferred 28 clusters with 15 clusters populated\n",
      "In stage 30: transferred 28 clusters with 15 clusters populated\n",
      "In stage 31: transferred 29 clusters with 15 clusters populated\n",
      "In stage 32: transferred 29 clusters with 16 clusters populated\n",
      "In stage 33: transferred 25 clusters with 16 clusters populated\n",
      "In stage 34: transferred 22 clusters with 16 clusters populated\n",
      "In stage 35: transferred 23 clusters with 16 clusters populated\n",
      "In stage 36: transferred 26 clusters with 16 clusters populated\n",
      "In stage 37: transferred 28 clusters with 14 clusters populated\n",
      "In stage 38: transferred 29 clusters with 16 clusters populated\n",
      "In stage 39: transferred 26 clusters with 15 clusters populated\n",
      "In stage 40: transferred 20 clusters with 15 clusters populated\n",
      "In stage 41: transferred 25 clusters with 16 clusters populated\n",
      "In stage 42: transferred 24 clusters with 15 clusters populated\n",
      "In stage 43: transferred 21 clusters with 14 clusters populated\n",
      "In stage 44: transferred 23 clusters with 16 clusters populated\n",
      "In stage 45: transferred 26 clusters with 17 clusters populated\n",
      "In stage 46: transferred 26 clusters with 15 clusters populated\n",
      "In stage 47: transferred 20 clusters with 16 clusters populated\n",
      "In stage 48: transferred 19 clusters with 16 clusters populated\n",
      "In stage 49: transferred 18 clusters with 15 clusters populated\n",
      "In stage 50: transferred 15 clusters with 16 clusters populated\n",
      "In stage 51: transferred 21 clusters with 16 clusters populated\n",
      "In stage 52: transferred 23 clusters with 15 clusters populated\n",
      "In stage 53: transferred 19 clusters with 15 clusters populated\n",
      "In stage 54: transferred 20 clusters with 16 clusters populated\n",
      "In stage 55: transferred 23 clusters with 16 clusters populated\n",
      "In stage 56: transferred 21 clusters with 16 clusters populated\n",
      "In stage 57: transferred 25 clusters with 16 clusters populated\n",
      "In stage 58: transferred 26 clusters with 16 clusters populated\n",
      "In stage 59: transferred 22 clusters with 15 clusters populated\n",
      "In stage 60: transferred 24 clusters with 15 clusters populated\n",
      "In stage 61: transferred 26 clusters with 16 clusters populated\n",
      "In stage 62: transferred 18 clusters with 17 clusters populated\n",
      "In stage 63: transferred 16 clusters with 16 clusters populated\n",
      "In stage 64: transferred 18 clusters with 14 clusters populated\n",
      "In stage 65: transferred 27 clusters with 16 clusters populated\n",
      "In stage 66: transferred 30 clusters with 16 clusters populated\n",
      "In stage 67: transferred 29 clusters with 15 clusters populated\n",
      "In stage 68: transferred 25 clusters with 16 clusters populated\n",
      "In stage 69: transferred 26 clusters with 17 clusters populated\n",
      "In stage 70: transferred 19 clusters with 17 clusters populated\n",
      "In stage 71: transferred 15 clusters with 15 clusters populated\n",
      "In stage 72: transferred 19 clusters with 15 clusters populated\n",
      "In stage 73: transferred 27 clusters with 17 clusters populated\n",
      "In stage 74: transferred 25 clusters with 16 clusters populated\n",
      "In stage 75: transferred 23 clusters with 14 clusters populated\n",
      "In stage 76: transferred 19 clusters with 14 clusters populated\n",
      "In stage 77: transferred 19 clusters with 16 clusters populated\n",
      "In stage 78: transferred 21 clusters with 16 clusters populated\n",
      "In stage 79: transferred 21 clusters with 15 clusters populated\n",
      "In stage 80: transferred 19 clusters with 15 clusters populated\n",
      "In stage 81: transferred 20 clusters with 15 clusters populated\n",
      "In stage 82: transferred 28 clusters with 16 clusters populated\n",
      "In stage 83: transferred 25 clusters with 16 clusters populated\n",
      "In stage 84: transferred 27 clusters with 16 clusters populated\n",
      "In stage 85: transferred 26 clusters with 15 clusters populated\n",
      "In stage 86: transferred 26 clusters with 16 clusters populated\n",
      "In stage 87: transferred 25 clusters with 14 clusters populated\n",
      "In stage 88: transferred 24 clusters with 16 clusters populated\n",
      "In stage 89: transferred 24 clusters with 16 clusters populated\n",
      "In stage 90: transferred 19 clusters with 15 clusters populated\n",
      "In stage 91: transferred 17 clusters with 15 clusters populated\n",
      "In stage 92: transferred 23 clusters with 15 clusters populated\n",
      "In stage 93: transferred 23 clusters with 17 clusters populated\n",
      "In stage 94: transferred 29 clusters with 16 clusters populated\n",
      "In stage 95: transferred 24 clusters with 14 clusters populated\n",
      "In stage 96: transferred 25 clusters with 15 clusters populated\n",
      "In stage 97: transferred 27 clusters with 15 clusters populated\n",
      "In stage 98: transferred 28 clusters with 14 clusters populated\n",
      "In stage 99: transferred 23 clusters with 15 clusters populated\n",
      "In stage 100: transferred 28 clusters with 16 clusters populated\n",
      "In stage 101: transferred 29 clusters with 14 clusters populated\n",
      "In stage 102: transferred 26 clusters with 15 clusters populated\n",
      "In stage 103: transferred 19 clusters with 16 clusters populated\n",
      "In stage 104: transferred 20 clusters with 16 clusters populated\n",
      "In stage 105: transferred 17 clusters with 16 clusters populated\n",
      "In stage 106: transferred 15 clusters with 15 clusters populated\n",
      "In stage 107: transferred 22 clusters with 15 clusters populated\n",
      "In stage 108: transferred 20 clusters with 15 clusters populated\n",
      "In stage 109: transferred 11 clusters with 15 clusters populated\n",
      "In stage 110: transferred 18 clusters with 15 clusters populated\n",
      "In stage 111: transferred 26 clusters with 15 clusters populated\n",
      "In stage 112: transferred 29 clusters with 15 clusters populated\n",
      "In stage 113: transferred 22 clusters with 13 clusters populated\n",
      "In stage 114: transferred 21 clusters with 16 clusters populated\n",
      "In stage 115: transferred 25 clusters with 16 clusters populated\n",
      "In stage 116: transferred 25 clusters with 14 clusters populated\n",
      "In stage 117: transferred 23 clusters with 15 clusters populated\n",
      "In stage 118: transferred 23 clusters with 14 clusters populated\n",
      "In stage 119: transferred 25 clusters with 16 clusters populated\n",
      "In stage 120: transferred 22 clusters with 15 clusters populated\n",
      "In stage 121: transferred 29 clusters with 15 clusters populated\n",
      "In stage 122: transferred 21 clusters with 15 clusters populated\n",
      "In stage 123: transferred 24 clusters with 14 clusters populated\n",
      "In stage 124: transferred 23 clusters with 16 clusters populated\n",
      "In stage 125: transferred 25 clusters with 15 clusters populated\n",
      "In stage 126: transferred 24 clusters with 17 clusters populated\n",
      "In stage 127: transferred 17 clusters with 16 clusters populated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 128: transferred 20 clusters with 14 clusters populated\n",
      "In stage 129: transferred 19 clusters with 14 clusters populated\n",
      "In stage 130: transferred 27 clusters with 15 clusters populated\n",
      "In stage 131: transferred 19 clusters with 16 clusters populated\n",
      "In stage 132: transferred 15 clusters with 15 clusters populated\n",
      "In stage 133: transferred 23 clusters with 14 clusters populated\n",
      "In stage 134: transferred 23 clusters with 15 clusters populated\n",
      "In stage 135: transferred 25 clusters with 15 clusters populated\n",
      "In stage 136: transferred 16 clusters with 15 clusters populated\n",
      "In stage 137: transferred 21 clusters with 15 clusters populated\n",
      "In stage 138: transferred 23 clusters with 15 clusters populated\n",
      "In stage 139: transferred 23 clusters with 15 clusters populated\n",
      "In stage 140: transferred 19 clusters with 15 clusters populated\n",
      "In stage 141: transferred 19 clusters with 15 clusters populated\n",
      "In stage 142: transferred 24 clusters with 13 clusters populated\n",
      "In stage 143: transferred 24 clusters with 16 clusters populated\n",
      "In stage 144: transferred 15 clusters with 16 clusters populated\n",
      "In stage 145: transferred 12 clusters with 15 clusters populated\n",
      "In stage 146: transferred 14 clusters with 16 clusters populated\n",
      "In stage 147: transferred 18 clusters with 15 clusters populated\n",
      "In stage 148: transferred 26 clusters with 16 clusters populated\n",
      "In stage 149: transferred 27 clusters with 17 clusters populated\n",
      "In stage 150: transferred 21 clusters with 14 clusters populated\n",
      "In stage 151: transferred 13 clusters with 14 clusters populated\n",
      "In stage 152: transferred 19 clusters with 15 clusters populated\n",
      "In stage 153: transferred 23 clusters with 15 clusters populated\n",
      "In stage 154: transferred 16 clusters with 16 clusters populated\n",
      "In stage 155: transferred 17 clusters with 15 clusters populated\n",
      "In stage 156: transferred 23 clusters with 16 clusters populated\n",
      "In stage 157: transferred 26 clusters with 15 clusters populated\n",
      "In stage 158: transferred 30 clusters with 16 clusters populated\n",
      "In stage 159: transferred 30 clusters with 15 clusters populated\n",
      "In stage 160: transferred 30 clusters with 15 clusters populated\n",
      "In stage 161: transferred 29 clusters with 14 clusters populated\n",
      "In stage 162: transferred 23 clusters with 15 clusters populated\n",
      "In stage 163: transferred 22 clusters with 15 clusters populated\n",
      "In stage 164: transferred 22 clusters with 16 clusters populated\n",
      "In stage 165: transferred 25 clusters with 16 clusters populated\n",
      "In stage 166: transferred 24 clusters with 15 clusters populated\n",
      "In stage 167: transferred 22 clusters with 15 clusters populated\n",
      "In stage 168: transferred 26 clusters with 15 clusters populated\n",
      "In stage 169: transferred 23 clusters with 16 clusters populated\n",
      "In stage 170: transferred 22 clusters with 16 clusters populated\n",
      "In stage 171: transferred 24 clusters with 16 clusters populated\n",
      "In stage 172: transferred 28 clusters with 15 clusters populated\n",
      "In stage 173: transferred 26 clusters with 16 clusters populated\n",
      "In stage 174: transferred 23 clusters with 16 clusters populated\n",
      "In stage 175: transferred 19 clusters with 17 clusters populated\n",
      "In stage 176: transferred 23 clusters with 17 clusters populated\n",
      "In stage 177: transferred 20 clusters with 15 clusters populated\n",
      "In stage 178: transferred 22 clusters with 16 clusters populated\n",
      "In stage 179: transferred 21 clusters with 15 clusters populated\n",
      "In stage 180: transferred 20 clusters with 16 clusters populated\n",
      "In stage 181: transferred 25 clusters with 15 clusters populated\n",
      "In stage 182: transferred 19 clusters with 16 clusters populated\n",
      "In stage 183: transferred 14 clusters with 14 clusters populated\n",
      "In stage 184: transferred 18 clusters with 15 clusters populated\n",
      "In stage 185: transferred 19 clusters with 16 clusters populated\n",
      "In stage 186: transferred 26 clusters with 16 clusters populated\n",
      "In stage 187: transferred 27 clusters with 17 clusters populated\n",
      "In stage 188: transferred 26 clusters with 14 clusters populated\n",
      "In stage 189: transferred 19 clusters with 15 clusters populated\n",
      "In stage 190: transferred 20 clusters with 14 clusters populated\n",
      "In stage 191: transferred 19 clusters with 15 clusters populated\n",
      "In stage 192: transferred 25 clusters with 16 clusters populated\n",
      "In stage 193: transferred 26 clusters with 15 clusters populated\n",
      "In stage 194: transferred 26 clusters with 17 clusters populated\n",
      "In stage 195: transferred 23 clusters with 16 clusters populated\n",
      "In stage 196: transferred 24 clusters with 15 clusters populated\n",
      "In stage 197: transferred 19 clusters with 15 clusters populated\n",
      "In stage 198: transferred 15 clusters with 15 clusters populated\n",
      "In stage 199: transferred 21 clusters with 16 clusters populated\n",
      "26.675 90.34251144948318\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import operator\n",
    "model=MovieGroupProcess(K=30, alpha=0.01, beta=0.01, n_iters=200)\n",
    "processed_docs_short=[i[:140] for i in processed_docs]\n",
    "y = model.fit(processed_docs_short,len(set(reduce(lambda x,y:x+y,processed_docs_short))))\n",
    "score_list=[model.choose_best_label(i) for i in processed_docs_short]\n",
    "topic_list=list(reversed([sorted(x.items(), key=operator.itemgetter(1))[-5:] for x in model.cluster_word_distribution if len(x)]))\n",
    "remove_list=remove_extrame(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('societi', 235),\n",
       "  ('say', 243),\n",
       "  ('mrcs', 407),\n",
       "  ('malawi', 420),\n",
       "  ('cross', 501)],\n",
       " [('ntcheu', 2), ('support', 2), ('cook', 2), ('mrcs', 3), ('teacher', 3)],\n",
       " [('malawi', 2), ('plant', 2), ('river', 2), ('tree', 3), ('cross', 6)],\n",
       " [('free', 1), ('breast', 1), ('cervic', 1), ('screen', 1), ('cancer', 2)],\n",
       " [('dropout', 1),\n",
       "  ('machinga', 1),\n",
       "  ('district', 1),\n",
       "  ('liwond', 1),\n",
       "  ('youth', 2)],\n",
       " [('campaign', 3), ('flood', 3), ('comment', 3), ('reloc', 3), ('need', 3)],\n",
       " [('alert', 4), ('heavi', 4), ('friend', 4), ('peopl', 4), ('rain', 9)],\n",
       " [('soil', 2), ('block', 2), ('cross', 3), ('zomba', 4), ('train', 5)],\n",
       " [('cross', 8), ('mrcs', 8), ('graphic', 10), ('train', 14), ('facilit', 18)],\n",
       " [('affect', 5), ('mame', 5), ('hire', 5), ('support', 6), ('donat', 7)],\n",
       " [('dodma', 8), ('organis', 8), ('commemor', 9), ('world', 9), ('mrcs', 24)],\n",
       " [('villag', 3), ('father', 3), ('flood', 5), ('water', 6), ('hous', 6)],\n",
       " [('servic', 2), ('talli', 2), ('count', 2), ('vote', 2), ('center', 3)],\n",
       " [('multi', 1), ('disabl', 1), ('hope', 1), ('need', 1), ('easter', 3)],\n",
       " [('imparti', 4),\n",
       "  ('cross', 5),\n",
       "  ('crescent', 5),\n",
       "  ('movement', 7),\n",
       "  ('human', 9)],\n",
       " [('cross', 3), ('societi', 3), ('concert', 3), ('flood', 4), ('malawi', 5)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_docs=[[word for word in doc if not word in remove_list] for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 253 clusters with 28 clusters populated\n",
      "In stage 1: transferred 97 clusters with 20 clusters populated\n",
      "In stage 2: transferred 44 clusters with 14 clusters populated\n",
      "In stage 3: transferred 48 clusters with 15 clusters populated\n",
      "In stage 4: transferred 44 clusters with 13 clusters populated\n",
      "In stage 5: transferred 36 clusters with 12 clusters populated\n",
      "In stage 6: transferred 33 clusters with 14 clusters populated\n",
      "In stage 7: transferred 39 clusters with 17 clusters populated\n",
      "In stage 8: transferred 29 clusters with 14 clusters populated\n",
      "In stage 9: transferred 32 clusters with 16 clusters populated\n",
      "In stage 10: transferred 32 clusters with 14 clusters populated\n",
      "In stage 11: transferred 27 clusters with 14 clusters populated\n",
      "In stage 12: transferred 37 clusters with 16 clusters populated\n",
      "In stage 13: transferred 43 clusters with 14 clusters populated\n",
      "In stage 14: transferred 42 clusters with 13 clusters populated\n",
      "In stage 15: transferred 37 clusters with 15 clusters populated\n",
      "In stage 16: transferred 44 clusters with 15 clusters populated\n",
      "In stage 17: transferred 41 clusters with 15 clusters populated\n",
      "In stage 18: transferred 40 clusters with 15 clusters populated\n",
      "In stage 19: transferred 38 clusters with 16 clusters populated\n",
      "In stage 20: transferred 34 clusters with 14 clusters populated\n",
      "In stage 21: transferred 30 clusters with 13 clusters populated\n",
      "In stage 22: transferred 35 clusters with 15 clusters populated\n",
      "In stage 23: transferred 40 clusters with 14 clusters populated\n",
      "In stage 24: transferred 42 clusters with 14 clusters populated\n",
      "In stage 25: transferred 37 clusters with 18 clusters populated\n",
      "In stage 26: transferred 44 clusters with 16 clusters populated\n",
      "In stage 27: transferred 37 clusters with 14 clusters populated\n",
      "In stage 28: transferred 37 clusters with 13 clusters populated\n",
      "In stage 29: transferred 36 clusters with 17 clusters populated\n",
      "In stage 30: transferred 29 clusters with 14 clusters populated\n",
      "In stage 31: transferred 28 clusters with 14 clusters populated\n",
      "In stage 32: transferred 37 clusters with 15 clusters populated\n",
      "In stage 33: transferred 34 clusters with 13 clusters populated\n",
      "In stage 34: transferred 35 clusters with 15 clusters populated\n",
      "In stage 35: transferred 32 clusters with 16 clusters populated\n",
      "In stage 36: transferred 37 clusters with 13 clusters populated\n",
      "In stage 37: transferred 37 clusters with 15 clusters populated\n",
      "In stage 38: transferred 38 clusters with 11 clusters populated\n",
      "In stage 39: transferred 38 clusters with 13 clusters populated\n",
      "In stage 40: transferred 38 clusters with 12 clusters populated\n",
      "In stage 41: transferred 40 clusters with 13 clusters populated\n",
      "In stage 42: transferred 40 clusters with 14 clusters populated\n",
      "In stage 43: transferred 45 clusters with 16 clusters populated\n",
      "In stage 44: transferred 39 clusters with 16 clusters populated\n",
      "In stage 45: transferred 38 clusters with 13 clusters populated\n",
      "In stage 46: transferred 34 clusters with 15 clusters populated\n",
      "In stage 47: transferred 31 clusters with 14 clusters populated\n",
      "In stage 48: transferred 28 clusters with 14 clusters populated\n",
      "In stage 49: transferred 33 clusters with 15 clusters populated\n",
      "In stage 50: transferred 44 clusters with 15 clusters populated\n",
      "In stage 51: transferred 37 clusters with 15 clusters populated\n",
      "In stage 52: transferred 31 clusters with 12 clusters populated\n",
      "In stage 53: transferred 40 clusters with 15 clusters populated\n",
      "In stage 54: transferred 37 clusters with 15 clusters populated\n",
      "In stage 55: transferred 38 clusters with 15 clusters populated\n",
      "In stage 56: transferred 38 clusters with 14 clusters populated\n",
      "In stage 57: transferred 39 clusters with 15 clusters populated\n",
      "In stage 58: transferred 41 clusters with 15 clusters populated\n",
      "In stage 59: transferred 36 clusters with 14 clusters populated\n",
      "In stage 60: transferred 33 clusters with 16 clusters populated\n",
      "In stage 61: transferred 38 clusters with 14 clusters populated\n",
      "In stage 62: transferred 38 clusters with 14 clusters populated\n",
      "In stage 63: transferred 30 clusters with 12 clusters populated\n",
      "In stage 64: transferred 30 clusters with 14 clusters populated\n",
      "In stage 65: transferred 29 clusters with 13 clusters populated\n",
      "In stage 66: transferred 33 clusters with 14 clusters populated\n",
      "In stage 67: transferred 38 clusters with 15 clusters populated\n",
      "In stage 68: transferred 38 clusters with 17 clusters populated\n",
      "In stage 69: transferred 32 clusters with 13 clusters populated\n",
      "In stage 70: transferred 33 clusters with 15 clusters populated\n",
      "In stage 71: transferred 34 clusters with 15 clusters populated\n",
      "In stage 72: transferred 38 clusters with 14 clusters populated\n",
      "In stage 73: transferred 38 clusters with 14 clusters populated\n",
      "In stage 74: transferred 30 clusters with 13 clusters populated\n",
      "In stage 75: transferred 28 clusters with 12 clusters populated\n",
      "In stage 76: transferred 31 clusters with 14 clusters populated\n",
      "In stage 77: transferred 37 clusters with 15 clusters populated\n",
      "In stage 78: transferred 35 clusters with 15 clusters populated\n",
      "In stage 79: transferred 42 clusters with 14 clusters populated\n",
      "In stage 80: transferred 37 clusters with 13 clusters populated\n",
      "In stage 81: transferred 36 clusters with 15 clusters populated\n",
      "In stage 82: transferred 37 clusters with 15 clusters populated\n",
      "In stage 83: transferred 31 clusters with 11 clusters populated\n",
      "In stage 84: transferred 30 clusters with 14 clusters populated\n",
      "In stage 85: transferred 38 clusters with 14 clusters populated\n",
      "In stage 86: transferred 42 clusters with 15 clusters populated\n",
      "In stage 87: transferred 36 clusters with 14 clusters populated\n",
      "In stage 88: transferred 35 clusters with 15 clusters populated\n",
      "In stage 89: transferred 29 clusters with 14 clusters populated\n",
      "In stage 90: transferred 33 clusters with 15 clusters populated\n",
      "In stage 91: transferred 33 clusters with 14 clusters populated\n",
      "In stage 92: transferred 34 clusters with 14 clusters populated\n",
      "In stage 93: transferred 33 clusters with 14 clusters populated\n",
      "In stage 94: transferred 34 clusters with 14 clusters populated\n",
      "In stage 95: transferred 35 clusters with 17 clusters populated\n",
      "In stage 96: transferred 32 clusters with 13 clusters populated\n",
      "In stage 97: transferred 28 clusters with 13 clusters populated\n",
      "In stage 98: transferred 33 clusters with 13 clusters populated\n",
      "In stage 99: transferred 33 clusters with 15 clusters populated\n",
      "In stage 100: transferred 31 clusters with 14 clusters populated\n",
      "In stage 101: transferred 39 clusters with 15 clusters populated\n",
      "In stage 102: transferred 34 clusters with 14 clusters populated\n",
      "In stage 103: transferred 29 clusters with 14 clusters populated\n",
      "In stage 104: transferred 33 clusters with 13 clusters populated\n",
      "In stage 105: transferred 22 clusters with 15 clusters populated\n",
      "In stage 106: transferred 39 clusters with 18 clusters populated\n",
      "In stage 107: transferred 39 clusters with 13 clusters populated\n",
      "In stage 108: transferred 33 clusters with 14 clusters populated\n",
      "In stage 109: transferred 33 clusters with 13 clusters populated\n",
      "In stage 110: transferred 36 clusters with 15 clusters populated\n",
      "In stage 111: transferred 35 clusters with 14 clusters populated\n",
      "In stage 112: transferred 42 clusters with 13 clusters populated\n",
      "In stage 113: transferred 38 clusters with 14 clusters populated\n",
      "In stage 114: transferred 38 clusters with 14 clusters populated\n",
      "In stage 115: transferred 34 clusters with 13 clusters populated\n",
      "In stage 116: transferred 31 clusters with 12 clusters populated\n",
      "In stage 117: transferred 32 clusters with 15 clusters populated\n",
      "In stage 118: transferred 39 clusters with 14 clusters populated\n",
      "In stage 119: transferred 40 clusters with 14 clusters populated\n",
      "In stage 120: transferred 31 clusters with 14 clusters populated\n",
      "In stage 121: transferred 41 clusters with 14 clusters populated\n",
      "In stage 122: transferred 37 clusters with 14 clusters populated\n",
      "In stage 123: transferred 34 clusters with 14 clusters populated\n",
      "In stage 124: transferred 36 clusters with 16 clusters populated\n",
      "In stage 125: transferred 34 clusters with 13 clusters populated\n",
      "In stage 126: transferred 35 clusters with 14 clusters populated\n",
      "In stage 127: transferred 35 clusters with 14 clusters populated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 128: transferred 39 clusters with 12 clusters populated\n",
      "In stage 129: transferred 42 clusters with 12 clusters populated\n",
      "In stage 130: transferred 40 clusters with 14 clusters populated\n",
      "In stage 131: transferred 33 clusters with 15 clusters populated\n",
      "In stage 132: transferred 36 clusters with 12 clusters populated\n",
      "In stage 133: transferred 24 clusters with 14 clusters populated\n",
      "In stage 134: transferred 33 clusters with 14 clusters populated\n",
      "In stage 135: transferred 37 clusters with 13 clusters populated\n",
      "In stage 136: transferred 31 clusters with 15 clusters populated\n",
      "In stage 137: transferred 26 clusters with 17 clusters populated\n",
      "In stage 138: transferred 34 clusters with 16 clusters populated\n",
      "In stage 139: transferred 32 clusters with 14 clusters populated\n",
      "In stage 140: transferred 33 clusters with 14 clusters populated\n",
      "In stage 141: transferred 39 clusters with 14 clusters populated\n",
      "In stage 142: transferred 34 clusters with 15 clusters populated\n",
      "In stage 143: transferred 42 clusters with 13 clusters populated\n",
      "In stage 144: transferred 39 clusters with 15 clusters populated\n",
      "In stage 145: transferred 38 clusters with 16 clusters populated\n",
      "In stage 146: transferred 30 clusters with 13 clusters populated\n",
      "In stage 147: transferred 40 clusters with 14 clusters populated\n",
      "In stage 148: transferred 41 clusters with 13 clusters populated\n",
      "In stage 149: transferred 41 clusters with 13 clusters populated\n",
      "In stage 150: transferred 40 clusters with 13 clusters populated\n",
      "In stage 151: transferred 37 clusters with 17 clusters populated\n",
      "In stage 152: transferred 36 clusters with 16 clusters populated\n",
      "In stage 153: transferred 38 clusters with 17 clusters populated\n",
      "In stage 154: transferred 34 clusters with 12 clusters populated\n",
      "In stage 155: transferred 38 clusters with 15 clusters populated\n",
      "In stage 156: transferred 43 clusters with 18 clusters populated\n",
      "In stage 157: transferred 40 clusters with 16 clusters populated\n",
      "In stage 158: transferred 38 clusters with 16 clusters populated\n",
      "In stage 159: transferred 32 clusters with 13 clusters populated\n",
      "In stage 160: transferred 30 clusters with 13 clusters populated\n",
      "In stage 161: transferred 33 clusters with 13 clusters populated\n",
      "In stage 162: transferred 39 clusters with 15 clusters populated\n",
      "In stage 163: transferred 41 clusters with 17 clusters populated\n",
      "In stage 164: transferred 42 clusters with 14 clusters populated\n",
      "In stage 165: transferred 41 clusters with 12 clusters populated\n",
      "In stage 166: transferred 32 clusters with 14 clusters populated\n",
      "In stage 167: transferred 40 clusters with 15 clusters populated\n",
      "In stage 168: transferred 40 clusters with 13 clusters populated\n",
      "In stage 169: transferred 40 clusters with 16 clusters populated\n",
      "In stage 170: transferred 41 clusters with 15 clusters populated\n",
      "In stage 171: transferred 40 clusters with 15 clusters populated\n",
      "In stage 172: transferred 37 clusters with 14 clusters populated\n",
      "In stage 173: transferred 37 clusters with 15 clusters populated\n",
      "In stage 174: transferred 38 clusters with 14 clusters populated\n",
      "In stage 175: transferred 39 clusters with 15 clusters populated\n",
      "In stage 176: transferred 33 clusters with 13 clusters populated\n",
      "In stage 177: transferred 38 clusters with 15 clusters populated\n",
      "In stage 178: transferred 43 clusters with 14 clusters populated\n",
      "In stage 179: transferred 38 clusters with 14 clusters populated\n",
      "In stage 180: transferred 37 clusters with 12 clusters populated\n",
      "In stage 181: transferred 36 clusters with 13 clusters populated\n",
      "In stage 182: transferred 33 clusters with 14 clusters populated\n",
      "In stage 183: transferred 31 clusters with 13 clusters populated\n",
      "In stage 184: transferred 30 clusters with 14 clusters populated\n",
      "In stage 185: transferred 34 clusters with 14 clusters populated\n",
      "In stage 186: transferred 39 clusters with 16 clusters populated\n",
      "In stage 187: transferred 42 clusters with 13 clusters populated\n",
      "In stage 188: transferred 38 clusters with 16 clusters populated\n",
      "In stage 189: transferred 36 clusters with 17 clusters populated\n",
      "In stage 190: transferred 35 clusters with 16 clusters populated\n",
      "In stage 191: transferred 38 clusters with 13 clusters populated\n",
      "In stage 192: transferred 34 clusters with 14 clusters populated\n",
      "In stage 193: transferred 34 clusters with 13 clusters populated\n",
      "In stage 194: transferred 35 clusters with 17 clusters populated\n",
      "In stage 195: transferred 41 clusters with 14 clusters populated\n",
      "In stage 196: transferred 36 clusters with 14 clusters populated\n",
      "In stage 197: transferred 33 clusters with 13 clusters populated\n",
      "In stage 198: transferred 38 clusters with 15 clusters populated\n",
      "In stage 199: transferred 36 clusters with 16 clusters populated\n",
      "[[('communiti', 151), ('support', 155), ('disast', 180), ('societi', 199), ('say', 243)], [('undp', 8), ('materogi', 8), ('dept', 8), ('dodma', 8), ('organis', 8)], [('mobil', 1), ('phone', 1), ('thankstoicrc', 1), ('power', 2), ('learn', 3)], [('conduct', 2), ('household', 2), ('children', 2), ('cancer', 2), ('campaign', 3)], [('valu', 3), ('discrimin', 3), ('crescent', 4), ('movement', 6), ('human', 8)], [('look', 1), ('ntcheu', 2), ('support', 2), ('cook', 2), ('teacher', 3)], [('dropout', 1), ('machinga', 1), ('district', 1), ('liwond', 1), ('youth', 2)], [('distribut', 16), ('launch', 16), ('presid', 18), ('blanket', 22), ('societi', 29)], [('victim', 6), ('ndifeamodzi', 7), ('societi', 8), ('flood', 8), ('donat', 17)], [('famili', 1), ('link', 1), ('offer', 1), ('servic', 1), ('river', 2)], [('water', 8), ('peopl', 8), ('hous', 10), ('rain', 11), ('flood', 13)], [('redcross', 5), ('center', 5), ('public', 5), ('workshop', 6), ('societi', 11)], [('passa', 6), ('participatori', 7), ('graphic', 10), ('train', 15), ('facilit', 18)]]\n"
     ]
    }
   ],
   "source": [
    "model=MovieGroupProcess(K=30, alpha=0.5, beta=0.1, n_iters=200)\n",
    "processed_docs_short=[i[:140] for i in tmp_docs]\n",
    "y = model.fit(processed_docs_short,len(set(reduce(lambda x,y:x+y,processed_docs_short))))\n",
    "score_list=[model.choose_best_label(i) for i in processed_docs_short]\n",
    "topic_list=list(reversed([sorted(x.items(), key=operator.itemgetter(1))[-5:] for x in model.cluster_word_distribution if len(x)]))\n",
    "print(topic_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(communiti, 151)</td>\n",
       "      <td>(support, 155)</td>\n",
       "      <td>(disast, 180)</td>\n",
       "      <td>(societi, 199)</td>\n",
       "      <td>(say, 243)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(undp, 8)</td>\n",
       "      <td>(materogi, 8)</td>\n",
       "      <td>(dept, 8)</td>\n",
       "      <td>(dodma, 8)</td>\n",
       "      <td>(organis, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(mobil, 1)</td>\n",
       "      <td>(phone, 1)</td>\n",
       "      <td>(thankstoicrc, 1)</td>\n",
       "      <td>(power, 2)</td>\n",
       "      <td>(learn, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(conduct, 2)</td>\n",
       "      <td>(household, 2)</td>\n",
       "      <td>(children, 2)</td>\n",
       "      <td>(cancer, 2)</td>\n",
       "      <td>(campaign, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(valu, 3)</td>\n",
       "      <td>(discrimin, 3)</td>\n",
       "      <td>(crescent, 4)</td>\n",
       "      <td>(movement, 6)</td>\n",
       "      <td>(human, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(look, 1)</td>\n",
       "      <td>(ntcheu, 2)</td>\n",
       "      <td>(support, 2)</td>\n",
       "      <td>(cook, 2)</td>\n",
       "      <td>(teacher, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(dropout, 1)</td>\n",
       "      <td>(machinga, 1)</td>\n",
       "      <td>(district, 1)</td>\n",
       "      <td>(liwond, 1)</td>\n",
       "      <td>(youth, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(distribut, 16)</td>\n",
       "      <td>(launch, 16)</td>\n",
       "      <td>(presid, 18)</td>\n",
       "      <td>(blanket, 22)</td>\n",
       "      <td>(societi, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>(victim, 6)</td>\n",
       "      <td>(ndifeamodzi, 7)</td>\n",
       "      <td>(societi, 8)</td>\n",
       "      <td>(flood, 8)</td>\n",
       "      <td>(donat, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>(famili, 1)</td>\n",
       "      <td>(link, 1)</td>\n",
       "      <td>(offer, 1)</td>\n",
       "      <td>(servic, 1)</td>\n",
       "      <td>(river, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>(water, 8)</td>\n",
       "      <td>(peopl, 8)</td>\n",
       "      <td>(hous, 10)</td>\n",
       "      <td>(rain, 11)</td>\n",
       "      <td>(flood, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>(redcross, 5)</td>\n",
       "      <td>(center, 5)</td>\n",
       "      <td>(public, 5)</td>\n",
       "      <td>(workshop, 6)</td>\n",
       "      <td>(societi, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>(passa, 6)</td>\n",
       "      <td>(participatori, 7)</td>\n",
       "      <td>(graphic, 10)</td>\n",
       "      <td>(train, 15)</td>\n",
       "      <td>(facilit, 18)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                   1                  2               3  \\\n",
       "0   (communiti, 151)      (support, 155)      (disast, 180)  (societi, 199)   \n",
       "1          (undp, 8)       (materogi, 8)          (dept, 8)      (dodma, 8)   \n",
       "2         (mobil, 1)          (phone, 1)  (thankstoicrc, 1)      (power, 2)   \n",
       "3       (conduct, 2)      (household, 2)      (children, 2)     (cancer, 2)   \n",
       "4          (valu, 3)      (discrimin, 3)      (crescent, 4)   (movement, 6)   \n",
       "5          (look, 1)         (ntcheu, 2)       (support, 2)       (cook, 2)   \n",
       "6       (dropout, 1)       (machinga, 1)      (district, 1)     (liwond, 1)   \n",
       "7    (distribut, 16)        (launch, 16)       (presid, 18)   (blanket, 22)   \n",
       "8        (victim, 6)    (ndifeamodzi, 7)       (societi, 8)      (flood, 8)   \n",
       "9        (famili, 1)           (link, 1)         (offer, 1)     (servic, 1)   \n",
       "10        (water, 8)          (peopl, 8)         (hous, 10)      (rain, 11)   \n",
       "11     (redcross, 5)         (center, 5)        (public, 5)   (workshop, 6)   \n",
       "12        (passa, 6)  (participatori, 7)      (graphic, 10)     (train, 15)   \n",
       "\n",
       "                4  \n",
       "0      (say, 243)  \n",
       "1    (organis, 8)  \n",
       "2      (learn, 3)  \n",
       "3   (campaign, 3)  \n",
       "4      (human, 8)  \n",
       "5    (teacher, 3)  \n",
       "6      (youth, 2)  \n",
       "7   (societi, 29)  \n",
       "8     (donat, 17)  \n",
       "9      (river, 2)  \n",
       "10    (flood, 13)  \n",
       "11  (societi, 11)  \n",
       "12  (facilit, 18)  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(topic_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bow approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    token_list=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            token_list.append(token)\n",
    "    return result,dict(zip(result,token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_ser=given_topic_df['description'].map(preprocess)\n",
    "processed_docs=[item[0] for item in processed_ser]\n",
    "mapping_list=[item[1] for item in processed_ser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "dictionary.filter_extremes()\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=dictionary.from_documents(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [], [], [], []]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI_topic_df=pd.DataFrame([[(mapping121_MI[mapping121_MI[0]==j[0]][1].values[0],j[1]) for j in i] for i in topic_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(communities, 151)</td>\n",
       "      <td>(support, 155)</td>\n",
       "      <td>(disaster, 180)</td>\n",
       "      <td>(society, 199)</td>\n",
       "      <td>(said, 243)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(undp, 8)</td>\n",
       "      <td>(materogy, 8)</td>\n",
       "      <td>(dept, 8)</td>\n",
       "      <td>(dodma, 8)</td>\n",
       "      <td>(organised, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(mobilization, 1)</td>\n",
       "      <td>(phone, 1)</td>\n",
       "      <td>(thankstoicrc, 1)</td>\n",
       "      <td>(power, 2)</td>\n",
       "      <td>(learn, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(conducted, 2)</td>\n",
       "      <td>(households, 2)</td>\n",
       "      <td>(children, 2)</td>\n",
       "      <td>(cancer, 2)</td>\n",
       "      <td>(campaign, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(value, 3)</td>\n",
       "      <td>(discrimination, 3)</td>\n",
       "      <td>(crescent, 4)</td>\n",
       "      <td>(movement, 6)</td>\n",
       "      <td>(human, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(looking, 1)</td>\n",
       "      <td>(ntcheu, 2)</td>\n",
       "      <td>(support, 2)</td>\n",
       "      <td>(cooking, 2)</td>\n",
       "      <td>(teacher, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(dropout, 1)</td>\n",
       "      <td>(machinga, 1)</td>\n",
       "      <td>(district, 1)</td>\n",
       "      <td>(liwonde, 1)</td>\n",
       "      <td>(youth, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(distribution, 16)</td>\n",
       "      <td>(launch, 16)</td>\n",
       "      <td>(president, 18)</td>\n",
       "      <td>(blankets, 22)</td>\n",
       "      <td>(society, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>(victims, 6)</td>\n",
       "      <td>(ndifeamodzi, 7)</td>\n",
       "      <td>(society, 8)</td>\n",
       "      <td>(floods, 8)</td>\n",
       "      <td>(donation, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>(families, 1)</td>\n",
       "      <td>(links, 1)</td>\n",
       "      <td>(offered, 1)</td>\n",
       "      <td>(services, 1)</td>\n",
       "      <td>(river, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>(water, 8)</td>\n",
       "      <td>(people, 8)</td>\n",
       "      <td>(houses, 10)</td>\n",
       "      <td>(rains, 11)</td>\n",
       "      <td>(floods, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>(redcross, 5)</td>\n",
       "      <td>(centers, 5)</td>\n",
       "      <td>(public, 5)</td>\n",
       "      <td>(workshop, 6)</td>\n",
       "      <td>(society, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>(passa, 6)</td>\n",
       "      <td>(participatory, 7)</td>\n",
       "      <td>(graphic, 10)</td>\n",
       "      <td>(training, 15)</td>\n",
       "      <td>(facilitators, 18)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                    1                  2  \\\n",
       "0   (communities, 151)       (support, 155)    (disaster, 180)   \n",
       "1            (undp, 8)        (materogy, 8)          (dept, 8)   \n",
       "2    (mobilization, 1)           (phone, 1)  (thankstoicrc, 1)   \n",
       "3       (conducted, 2)      (households, 2)      (children, 2)   \n",
       "4           (value, 3)  (discrimination, 3)      (crescent, 4)   \n",
       "5         (looking, 1)          (ntcheu, 2)       (support, 2)   \n",
       "6         (dropout, 1)        (machinga, 1)      (district, 1)   \n",
       "7   (distribution, 16)         (launch, 16)    (president, 18)   \n",
       "8         (victims, 6)     (ndifeamodzi, 7)       (society, 8)   \n",
       "9        (families, 1)           (links, 1)       (offered, 1)   \n",
       "10          (water, 8)          (people, 8)       (houses, 10)   \n",
       "11       (redcross, 5)         (centers, 5)        (public, 5)   \n",
       "12          (passa, 6)   (participatory, 7)      (graphic, 10)   \n",
       "\n",
       "                 3                   4  \n",
       "0   (society, 199)         (said, 243)  \n",
       "1       (dodma, 8)      (organised, 8)  \n",
       "2       (power, 2)          (learn, 3)  \n",
       "3      (cancer, 2)       (campaign, 3)  \n",
       "4    (movement, 6)          (human, 8)  \n",
       "5     (cooking, 2)        (teacher, 3)  \n",
       "6     (liwonde, 1)          (youth, 2)  \n",
       "7   (blankets, 22)       (society, 29)  \n",
       "8      (floods, 8)      (donation, 17)  \n",
       "9    (services, 1)          (river, 2)  \n",
       "10     (rains, 11)        (floods, 13)  \n",
       "11   (workshop, 6)       (society, 11)  \n",
       "12  (training, 15)  (facilitators, 18)  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem2original(row):\n",
    "    stem,_type=row['stem'],row['type']\n",
    "    if _type==1:\n",
    "        return mapping121_description[mapping121_description[0]==stem][1].values[0]\n",
    "    elif _type==2:\n",
    "        return mapping121_strategy[mapping121_strategy[0]==stem][1].values[0]\n",
    "    elif _type==3:\n",
    "        return mapping121_extra[mapping121_extra[0]==stem][1].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "given_topic['original_word']=given_topic.apply(lambda x:stem2original(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>stem</th>\n",
       "      <th>topic</th>\n",
       "      <th>type</th>\n",
       "      <th>original_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.177328</td>\n",
       "      <td>impact</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "      <td>impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.196265</td>\n",
       "      <td>level</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "      <td>levels</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.206380</td>\n",
       "      <td>need</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "      <td>need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.294397</td>\n",
       "      <td>vulner</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "      <td>vulnerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.412761</td>\n",
       "      <td>climat</td>\n",
       "      <td>climate change</td>\n",
       "      <td>1</td>\n",
       "      <td>climate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.211442</td>\n",
       "      <td>network</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "      <td>network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>organ</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "      <td>organizations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.234021</td>\n",
       "      <td>trust</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.281923</td>\n",
       "      <td>influenc</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "      <td>influence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.369124</td>\n",
       "      <td>govern</td>\n",
       "      <td>values, power and inclusion</td>\n",
       "      <td>3</td>\n",
       "      <td>governments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       score      stem                        topic  type  original_word\n",
       "0   0.177328    impact               climate change     1         impact\n",
       "1   0.196265     level               climate change     1         levels\n",
       "2   0.206380      need               climate change     1           need\n",
       "3   0.294397    vulner               climate change     1     vulnerable\n",
       "4   0.412761    climat               climate change     1        climate\n",
       "..       ...       ...                          ...   ...            ...\n",
       "70  0.211442   network  values, power and inclusion     3        network\n",
       "71  0.234021     organ  values, power and inclusion     3  organizations\n",
       "72  0.234021     trust  values, power and inclusion     3          trust\n",
       "73  0.281923  influenc  values, power and inclusion     3      influence\n",
       "74  0.369124    govern  values, power and inclusion     3    governments\n",
       "\n",
       "[75 rows x 5 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weight_description=1.0\n",
    "weight_strategy=0.5\n",
    "weight_extra=0.3\n",
    "weight_list=[weight_description,weight_strategy,weight_extra]\n",
    "doc_count=list(reversed([i for i in model.cluster_doc_count if i]))\n",
    "def scoring(given_topic,social_media):\n",
    "    #tmp=social_media.copy()\n",
    "    #print(social_media.values)\n",
    "    score_list=[]\n",
    "    given_topic_list=given_topic['topic'].drop_duplicates().to_list()\n",
    "    for i in social_media.iterrows():\n",
    "        tmp=given_topic.copy()\n",
    "        tmp['score_topic']=given_topic.apply(lambda x:score_per_topic(x,i[1].tolist()),axis=1)\n",
    "        \n",
    "        score_list.append([i[0]]+tmp.groupby(by=['topic']).sum()[\"score_topic\"].tolist()+[doc_count[i[0]]]+[i[0] for i in social_media.values.tolist()[i[0]]])\n",
    "    return(pd.DataFrame(score_list,columns=[\"index\"]+given_topic_list+[\"NoDoc\"]+[\"key_word \"+str(i) for i in range(5,0,-1)]))\n",
    "#scoring(given_topic,MI_topic_df).to_csv(\"../data/MI_topic_matching.csv\")\n",
    "result=scoring(given_topic,MI_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrlllll}\n",
      "\\toprule\n",
      "{} &  index &  climate change &  crisis and disasters &  health &  migration and identity &  values, power and inclusion &  NoDoc &    key\\_word 5 &      key\\_word 4 &    key\\_word 3 & key\\_word 2 &    key\\_word 1 \\\\\n",
      "\\midrule\n",
      "0  &      0 &             0.0 &                 270.0 &     0.0 &                    45.3 &                          0.0 &    165 &   communities &         support &      disaster &    society &          said \\\\\n",
      "1  &      1 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      1 &          undp &        materogy &          dept &      dodma &     organised \\\\\n",
      "2  &      2 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      1 &  mobilization &           phone &  thankstoicrc &      power &         learn \\\\\n",
      "3  &      3 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      9 &     conducted &      households &      children &     cancer &      campaign \\\\\n",
      "4  &      4 &             2.4 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      2 &         value &  discrimination &      crescent &   movement &         human \\\\\n",
      "5  &      5 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      7 &       looking &          ntcheu &       support &    cooking &       teacher \\\\\n",
      "6  &      6 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      3 &       dropout &        machinga &      district &    liwonde &         youth \\\\\n",
      "7  &      7 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      1 &  distribution &          launch &     president &   blankets &       society \\\\\n",
      "8  &      8 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      1 &       victims &     ndifeamodzi &       society &     floods &      donation \\\\\n",
      "9  &      9 &             0.0 &                   0.0 &     0.5 &                     1.0 &                          0.0 &     28 &      families &           links &       offered &   services &         river \\\\\n",
      "10 &     10 &             0.0 &                   2.4 &     0.0 &                     0.0 &                          0.0 &     12 &         water &          people &        houses &      rains &        floods \\\\\n",
      "11 &     11 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &      3 &      redcross &         centers &        public &   workshop &       society \\\\\n",
      "12 &     12 &             0.0 &                   0.0 &     0.0 &                     0.0 &                          0.0 &     17 &         passa &   participatory &       graphic &   training &  facilitators \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "328.5"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def score_per_topic(row,word_list):\n",
    "    for word in word_list:\n",
    "        if fuzz.ratio(word[0],row['original_word'])>90:\n",
    "            return word[1]*weight_list[row['type']-1]\n",
    "    return 0\n",
    "given_topic.apply(lambda x:score_per_topic(x,[('people', 155), ('support', 167), ('disaster', 188), ('society', 238), ('said', 257)]),axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[165, 1, 1, 9, 2, 7, 3, 1, 1, 28, 12, 3, 17, 11, 3, 1]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed([i for i in model.cluster_doc_count if i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('communities', 151),\n",
       "  ('support', 155),\n",
       "  ('disaster', 180),\n",
       "  ('society', 199),\n",
       "  ('said', 243)],\n",
       " [('undp', 8), ('materogy', 8), ('dept', 8), ('dodma', 8), ('organised', 8)],\n",
       " [('mobilization', 1),\n",
       "  ('phone', 1),\n",
       "  ('thankstoicrc', 1),\n",
       "  ('power', 2),\n",
       "  ('learn', 3)],\n",
       " [('conducted', 2),\n",
       "  ('households', 2),\n",
       "  ('children', 2),\n",
       "  ('cancer', 2),\n",
       "  ('campaign', 3)],\n",
       " [('value', 3),\n",
       "  ('discrimination', 3),\n",
       "  ('crescent', 4),\n",
       "  ('movement', 6),\n",
       "  ('human', 8)],\n",
       " [('looking', 1),\n",
       "  ('ntcheu', 2),\n",
       "  ('support', 2),\n",
       "  ('cooking', 2),\n",
       "  ('teacher', 3)],\n",
       " [('dropout', 1),\n",
       "  ('machinga', 1),\n",
       "  ('district', 1),\n",
       "  ('liwonde', 1),\n",
       "  ('youth', 2)],\n",
       " [('distribution', 16),\n",
       "  ('launch', 16),\n",
       "  ('president', 18),\n",
       "  ('blankets', 22),\n",
       "  ('society', 29)],\n",
       " [('victims', 6),\n",
       "  ('ndifeamodzi', 7),\n",
       "  ('society', 8),\n",
       "  ('floods', 8),\n",
       "  ('donation', 17)],\n",
       " [('families', 1),\n",
       "  ('links', 1),\n",
       "  ('offered', 1),\n",
       "  ('services', 1),\n",
       "  ('river', 2)],\n",
       " [('water', 8), ('people', 8), ('houses', 10), ('rains', 11), ('floods', 13)],\n",
       " [('redcross', 5),\n",
       "  ('centers', 5),\n",
       "  ('public', 5),\n",
       "  ('workshop', 6),\n",
       "  ('society', 11)],\n",
       " [('passa', 6),\n",
       "  ('participatory', 7),\n",
       "  ('graphic', 10),\n",
       "  ('training', 15),\n",
       "  ('facilitators', 18)]]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_topic_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.config({\n",
       "    paths: {\n",
       "        d3: 'https://d3js.org/d3.v5.min'\n",
       "    }\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "    paths: {\n",
    "        d3: 'https://d3js.org/d3.v5.min'\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(element) {\n",
       "    require(['d3'], function(d3) {   \n",
       "        var data = [1, 2, 4, 8, 16, 8, 4, 2, 1]\n",
       "\n",
       "        var svg = d3.select(element.get(0)).append('svg')\n",
       "            .attr('width', 400)\n",
       "            .attr('height', 200);\n",
       "        svg.selectAll('circle')\n",
       "            .data(data)\n",
       "            .enter()\n",
       "            .append('circle')\n",
       "            .attr(\"cx\", function(d, i) {return 40 * (i + 1);})\n",
       "            .attr(\"cy\", function(d, i) {return 100 + 30 * (i % 3 - 1);})\n",
       "            .style(\"fill\", \"#1570a4\")\n",
       "            .transition().duration(2000)\n",
       "            .attr(\"r\", function(d) {return 2*d;})\n",
       "        ;\n",
       "    })\n",
       "})(element);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(element) {\n",
    "    require(['d3'], function(d3) {   \n",
    "        var data = [1, 2, 4, 8, 16, 8, 4, 2, 1]\n",
    "\n",
    "        var svg = d3.select(element.get(0)).append('svg')\n",
    "            .attr('width', 400)\n",
    "            .attr('height', 200);\n",
    "        svg.selectAll('circle')\n",
    "            .data(data)\n",
    "            .enter()\n",
    "            .append('circle')\n",
    "            .attr(\"cx\", function(d, i) {return 40 * (i + 1);})\n",
    "            .attr(\"cy\", function(d, i) {return 100 + 30 * (i % 3 - 1);})\n",
    "            .style(\"fill\", \"#1570a4\")\n",
    "            .transition().duration(2000)\n",
    "            .attr(\"r\", function(d) {return 2*d;})\n",
    "        ;\n",
    "    })\n",
    "})(element);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    index |   climate change |   crisis and disasters |   health |   migration and identity |   values, power and inclusion |   NoDoc | key_word 5   | key_word 4     | key_word 3   | key_word 2   | key_word 1   |\n",
      "| --------:|-----------------:|-----------------------:|---------:|-------------------------:|------------------------------:|--------:|:-------------|:---------------|:-------------|:-------------|:-------------|\n",
      "|        0 |              0   |                  270   |      0   |                     45.3 |                             0 |     165 | communities  | support        | disaster     | society      | said         |\n",
      "|        1 |              0   |                    0   |      0   |                      0   |                             0 |       1 | undp         | materogy       | dept         | dodma        | organised    |\n",
      "|        2 |              0   |                    0   |      0   |                      0   |                             0 |       1 | mobilization | phone          | thankstoicrc | power        | learn        |\n",
      "|        3 |              0   |                    0   |      0   |                      0   |                             0 |       9 | conducted    | households     | children     | cancer       | campaign     |\n",
      "|        4 |              2.4 |                    0   |      0   |                      0   |                             0 |       2 | value        | discrimination | crescent     | movement     | human        |\n",
      "|        5 |              0   |                    0   |      0   |                      0   |                             0 |       7 | looking      | ntcheu         | support      | cooking      | teacher      |\n",
      "|        6 |              0   |                    0   |      0   |                      0   |                             0 |       3 | dropout      | machinga       | district     | liwonde      | youth        |\n",
      "|        7 |              0   |                    0   |      0   |                      0   |                             0 |       1 | distribution | launch         | president    | blankets     | society      |\n",
      "|        8 |              0   |                    0   |      0   |                      0   |                             0 |       1 | victims      | ndifeamodzi    | society      | floods       | donation     |\n",
      "|        9 |              0   |                    0   |      0.5 |                      1   |                             0 |      28 | families     | links          | offered      | services     | river        |\n",
      "|       10 |              0   |                    2.4 |      0   |                      0   |                             0 |      12 | water        | people         | houses       | rains        | floods       |\n",
      "|       11 |              0   |                    0   |      0   |                      0   |                             0 |       3 | redcross     | centers        | public       | workshop     | society      |\n",
      "|       12 |              0   |                    0   |      0   |                      0   |                             0 |      17 | passa        | participatory  | graphic      | training     | facilitators |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def pandas_df_to_markdown_table(df):\n",
    "    # Dependent upon ipython\n",
    "    # shamelessly stolen from https://stackoverflow.com/questions/33181846/programmatically-convert-pandas-dataframe-to-markdown-table\n",
    "    from IPython.display import Markdown, display\n",
    "    fmt = ['---' for i in range(len(df.columns))]\n",
    "    df_fmt = pd.DataFrame([fmt], columns=df.columns)\n",
    "    df_formatted = pd.concat([df_fmt, df])\n",
    "    #display(Markdown(df_formatted.to_csv(sep=\"|\", index=False)))\n",
    "    return Markdown(df_formatted.to_csv(sep=\"|\", index=False))\n",
    "#     return df_formatted\n",
    "\n",
    "def df_to_markdown(df, y_index=False):\n",
    "    blob = tabulate(df, headers='keys', tablefmt='pipe')\n",
    "    if not y_index:\n",
    "        # Remove the index with some creative splicing and iteration\n",
    "        return '\\n'.join(['| {}'.format(row.split('|', 2)[-1]) for row in blob.split('\\n')])\n",
    "    return blob\n",
    "print(df_to_markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
